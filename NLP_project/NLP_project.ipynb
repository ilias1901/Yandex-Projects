{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiShop Comment Toxicity Classification Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "The objective of this project is to develop a machine learning model that classifies user comments as either positive or negative. We have access to a labeled dataset that includes information about the toxicity of these user edits.\n",
    "\n",
    "The primary goal is to build a model that achieves a minimum quality metric of an F1 score of 0.75.\n",
    "\n",
    "To successfully complete this project, we will follow these steps:\n",
    "\n",
    "1. Data Preparation\n",
    "Begin by loading and preparing the dataset provided.\n",
    "Ensure that the dataset is correctly formatted and that it contains the necessary columns: text (the comment text) and toxic (the toxicity label).\n",
    "2. Model Training\n",
    "Experiment with different machine learning models for comment classification.\n",
    "Consider trying various natural language processing techniques.\n",
    "Evaluate the models using relevant evaluation metrics.\n",
    "3. Conclusion\n",
    "Summarize our findings and observations from the model training process.\n",
    "Discuss the performance of different models and techniques.\n",
    "Reflect on the achievement of the project's goal: an F1 score of at least 0.75.\n",
    "\n",
    "Data Description\n",
    "The dataset required for this project is available in the file toxic_comments.csv. It consists of two main columns:\n",
    "\n",
    "`text`: This column contains the text of user comments.\n",
    "\n",
    "`toxic`: This column serves as the target attribute, indicating whether a comment is toxic (1) or not toxic (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./toxic_comments.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    143106\n",
       "1     16186\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset exhibits a class imbalance with a majority of non-toxic comments (143,106) and a minority of toxic comments (16,186)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Check for missing values and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "text          0\n",
       "toxic         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Let's prepare a function that will clean the text from unnecessary characters, convert all words to lowercase, and lemmatize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text): \n",
    "    cleared_text = re.sub(r'[^a-zA-Z]', ' ', text) \n",
    "    cleared_text = cleared_text.lower()\n",
    "    tokenized = nltk.word_tokenize(cleared_text)\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokenized]\n",
    "    return \" \".join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>d aww he match this background colour i m seem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hey man i m really not trying to edit war it s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>more i can t make any real suggestion on impro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  explanation why the edits made under my userna...      0\n",
       "1           1  d aww he match this background colour i m seem...      0\n",
       "2           2  hey man i m really not trying to edit war it s...      0\n",
       "3           3  more i can t make any real suggestion on impro...      0\n",
       "4           4  you sir are my hero any chance you remember wh...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(lemmatize_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Let's split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = train['text'].values.astype('U')\n",
    "corpus_test = test['text'].values.astype('U')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create matrices with TF-IDF values for the samples, while simultaneously removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "tf_idf_train = count_tf_idf.fit_transform(corpus_train)\n",
    "tf_idf_test = count_tf_idf.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = tf_idf_train\n",
    "target_train = train['toxic']\n",
    "\n",
    "features_test = tf_idf_test\n",
    "target_test = test['toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.5788533017523151\n",
      "Best params: {'class_weight': None, 'max_depth': 9}\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "param_grid = {'max_depth': range(1, 10, 2),\n",
    "    'class_weight': [None, 'balanced']}\n",
    "dt_grid = GridSearchCV(dt, param_grid=param_grid, cv=3, scoring='f1')\n",
    "dt_grid.fit(features_train, target_train)\n",
    "predict_train_dt = dt_grid.predict(features_train)\n",
    "best_score.append(dt_grid.best_score_)\n",
    "print('Best score:',dt_grid.best_score_)\n",
    "print('Best params:',dt_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.25097271789456593\n",
      "Best params: {'class_weight': 'balanced', 'max_depth': 6, 'n_estimators': 3}\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "params = {'n_estimators': range(1,5),\n",
    "         'max_depth': range(1,7),\n",
    "         'class_weight': [None, 'balanced']}\n",
    "rf_grid = GridSearchCV(rf, param_grid=params, cv=3, scoring='f1')\n",
    "rf_grid.fit(features_train, target_train)\n",
    "predict_train_rf = rf_grid.predict(features_train)\n",
    "best_score.append(rf_grid.best_score_)\n",
    "print('Best score:',rf_grid.best_score_)\n",
    "print('Best params:',rf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.7575055854259978\n",
      "Best params: {'class_weight': 'balanced', 'fit_intercept': True, 'multi_class': 'multinomial', 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "params = {'class_weight': [None, 'balanced'],\n",
    "         'solver': ['lbfgs','liblinear'],\n",
    "         'fit_intercept' : [True, False],\n",
    "         'multi_class': ['ovr','multinomial']}\n",
    "lr_grid = GridSearchCV(lr, param_grid=params,  cv=5, scoring='f1')\n",
    "lr_grid.fit(features_train, target_train)\n",
    "best_score.append(lr_grid.best_score_)\n",
    "print('Best score:',lr_grid.best_score_)\n",
    "print('Best params:',lr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision tree</th>\n",
       "      <td>0.578853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.250973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic regression</th>\n",
       "      <td>0.757506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     f1 score\n",
       "Decision tree        0.578853\n",
       "Random Forest        0.250973\n",
       "Logistic regression  0.757506"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conclusion = [best_score]\n",
    "index = ['f1 score']\n",
    "columns = ['Decision tree','Random Forest', 'Logistic regression'] \n",
    "df = pd.DataFrame(conclusion, index, columns) \n",
    "df.T.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The best result was demonstrated by Logistic regression. Let's test it on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7487982419997253\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(class_weight = 'balanced', fit_intercept = True, multi_class = 'multinomial', solver= 'lbfgs')\n",
    "lr.fit(features_train, target_train)\n",
    "predict = lr.predict(tf_idf_test)\n",
    "f1 = f1_score(target_test, predict)\n",
    "print (f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In conclusion, for this project, we experimented with various machine learning models to predict toxic comments. After thorough data preprocessing, including text cleaning and lemmatization, we trained and evaluated several models. Among them, the Logistic Regression model with balanced class weights, fit intercept, 'multinomial' multi-class handling, and 'lbfgs' solver achieved the highest F1 score of approximately 0.7488 on the test dataset. This suggests that the Logistic Regression model performed the best in identifying toxic comments in our dataset. Further fine-tuning and optimization could potentially improve the model's performance even more."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 363,
    "start_time": "2022-05-30T19:36:06.836Z"
   },
   {
    "duration": 2651,
    "start_time": "2022-05-30T19:37:28.425Z"
   },
   {
    "duration": 23,
    "start_time": "2022-05-30T19:37:53.487Z"
   },
   {
    "duration": 201,
    "start_time": "2022-05-30T19:40:31.562Z"
   },
   {
    "duration": 140,
    "start_time": "2022-05-30T19:41:28.301Z"
   },
   {
    "duration": 2,
    "start_time": "2022-05-30T19:53:48.019Z"
   },
   {
    "duration": 69,
    "start_time": "2022-05-30T19:54:00.104Z"
   },
   {
    "duration": 10,
    "start_time": "2022-05-30T19:54:10.744Z"
   },
   {
    "duration": 3592,
    "start_time": "2022-05-30T19:54:34.283Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-30T19:54:52.463Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-30T19:55:03.673Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-30T19:55:07.743Z"
   },
   {
    "duration": 10,
    "start_time": "2022-05-30T19:55:14.686Z"
   },
   {
    "duration": 867,
    "start_time": "2022-05-30T19:55:27.446Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-30T19:56:58.227Z"
   },
   {
    "duration": 371,
    "start_time": "2022-05-30T19:57:03.121Z"
   },
   {
    "duration": 762,
    "start_time": "2022-05-30T19:57:03.493Z"
   },
   {
    "duration": 18,
    "start_time": "2022-05-30T19:57:04.256Z"
   },
   {
    "duration": 206,
    "start_time": "2022-05-30T19:57:04.276Z"
   },
   {
    "duration": 1764,
    "start_time": "2022-05-30T19:57:04.484Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-30T19:57:06.250Z"
   },
   {
    "duration": 6,
    "start_time": "2022-05-30T19:57:06.257Z"
   },
   {
    "duration": 747,
    "start_time": "2022-05-30T19:57:06.265Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-30T19:57:55.413Z"
   },
   {
    "duration": 2,
    "start_time": "2022-05-30T19:58:12.386Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-30T19:58:15.881Z"
   },
   {
    "duration": 367,
    "start_time": "2022-05-30T19:58:36.620Z"
   },
   {
    "duration": 749,
    "start_time": "2022-05-30T19:58:36.992Z"
   },
   {
    "duration": 17,
    "start_time": "2022-05-30T19:58:37.742Z"
   },
   {
    "duration": 198,
    "start_time": "2022-05-30T19:58:37.761Z"
   },
   {
    "duration": 1782,
    "start_time": "2022-05-30T19:58:37.961Z"
   },
   {
    "duration": 8,
    "start_time": "2022-05-30T19:58:39.744Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-30T19:58:39.754Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-30T19:58:39.759Z"
   },
   {
    "duration": 26,
    "start_time": "2022-05-30T19:58:39.766Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-30T19:58:55.059Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-30T19:59:04.108Z"
   },
   {
    "duration": 2,
    "start_time": "2022-05-30T19:59:30.265Z"
   },
   {
    "duration": 733,
    "start_time": "2022-05-30T19:59:32.108Z"
   },
   {
    "duration": 802,
    "start_time": "2022-05-30T20:02:27.914Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-30T20:03:03.768Z"
   },
   {
    "duration": 689,
    "start_time": "2022-05-30T20:03:11.843Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-30T20:03:14.965Z"
   },
   {
    "duration": 111,
    "start_time": "2022-05-30T20:03:21.636Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-30T20:03:26.558Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-30T20:03:43.459Z"
   },
   {
    "duration": 11,
    "start_time": "2022-05-30T20:03:48.068Z"
   },
   {
    "duration": 1045,
    "start_time": "2022-05-30T20:03:53.859Z"
   },
   {
    "duration": 809,
    "start_time": "2022-05-30T20:03:54.905Z"
   },
   {
    "duration": 16,
    "start_time": "2022-05-30T20:03:55.716Z"
   },
   {
    "duration": 203,
    "start_time": "2022-05-30T20:03:55.733Z"
   },
   {
    "duration": 1743,
    "start_time": "2022-05-30T20:03:55.938Z"
   },
   {
    "duration": 10,
    "start_time": "2022-05-30T20:03:57.683Z"
   },
   {
    "duration": 5,
    "start_time": "2022-05-30T20:03:57.695Z"
   },
   {
    "duration": 7,
    "start_time": "2022-05-30T20:03:57.702Z"
   },
   {
    "duration": 710,
    "start_time": "2022-05-30T20:03:57.710Z"
   },
   {
    "duration": 117,
    "start_time": "2022-05-30T20:03:58.422Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-30T20:03:58.540Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-30T20:03:58.542Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-30T20:03:58.543Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-30T20:12:59.023Z"
   },
   {
    "duration": 8,
    "start_time": "2022-05-30T20:13:36.637Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-30T20:14:12.256Z"
   },
   {
    "duration": 3,
    "start_time": "2022-05-30T20:17:44.161Z"
   },
   {
    "duration": 9,
    "start_time": "2022-05-30T20:17:44.660Z"
   },
   {
    "duration": 124,
    "start_time": "2022-05-30T20:17:44.681Z"
   },
   {
    "duration": 10,
    "start_time": "2022-05-30T20:17:44.806Z"
   },
   {
    "duration": 0,
    "start_time": "2022-05-30T20:17:44.817Z"
   },
   {
    "duration": 9,
    "start_time": "2022-05-30T20:23:25.939Z"
   },
   {
    "duration": 980,
    "start_time": "2022-05-30T20:23:31.498Z"
   },
   {
    "duration": 773,
    "start_time": "2022-05-30T20:23:32.480Z"
   },
   {
    "duration": 15,
    "start_time": "2022-05-30T20:23:33.254Z"
   },
   {
    "duration": 209,
    "start_time": "2022-05-30T20:23:33.271Z"
   },
   {
    "duration": 1755,
    "start_time": "2022-05-30T20:23:33.481Z"
   },
   {
    "duration": 1088,
    "start_time": "2022-05-30T20:24:06.766Z"
   },
   {
    "duration": 2239,
    "start_time": "2022-05-30T20:24:07.855Z"
   },
   {
    "duration": 19,
    "start_time": "2022-05-30T20:24:10.096Z"
   },
   {
    "duration": 221,
    "start_time": "2022-05-30T20:24:10.116Z"
   },
   {
    "duration": 1794,
    "start_time": "2022-05-30T20:24:10.338Z"
   },
   {
    "duration": 1013,
    "start_time": "2022-05-30T20:26:47.562Z"
   },
   {
    "duration": 2233,
    "start_time": "2022-05-30T20:26:48.577Z"
   },
   {
    "duration": 22,
    "start_time": "2022-05-30T20:26:50.811Z"
   },
   {
    "duration": 195,
    "start_time": "2022-05-30T20:26:50.835Z"
   },
   {
    "duration": 1799,
    "start_time": "2022-05-30T20:26:51.032Z"
   },
   {
    "duration": 158,
    "start_time": "2022-05-30T20:26:52.833Z"
   },
   {
    "duration": 8484,
    "start_time": "2022-05-30T20:26:52.992Z"
   },
   {
    "duration": 4,
    "start_time": "2022-05-30T20:27:01.478Z"
   },
   {
    "duration": 1048,
    "start_time": "2022-06-01T19:41:54.380Z"
   },
   {
    "duration": 2297,
    "start_time": "2022-06-01T19:41:55.430Z"
   },
   {
    "duration": 23,
    "start_time": "2022-06-01T19:41:57.729Z"
   },
   {
    "duration": 217,
    "start_time": "2022-06-01T19:41:57.755Z"
   },
   {
    "duration": 1973,
    "start_time": "2022-06-01T19:41:57.973Z"
   },
   {
    "duration": 140,
    "start_time": "2022-06-01T19:41:59.948Z"
   },
   {
    "duration": 115,
    "start_time": "2022-06-01T19:42:00.090Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-01T19:42:00.206Z"
   },
   {
    "duration": 9209,
    "start_time": "2022-06-01T19:42:33.495Z"
   },
   {
    "duration": 6,
    "start_time": "2022-06-01T19:42:44.266Z"
   },
   {
    "duration": 3,
    "start_time": "2022-06-01T19:48:44.736Z"
   },
   {
    "duration": 27,
    "start_time": "2022-06-01T19:48:56.837Z"
   },
   {
    "duration": 1158,
    "start_time": "2022-06-01T19:52:45.584Z"
   },
   {
    "duration": 875,
    "start_time": "2022-06-01T19:52:48.884Z"
   },
   {
    "duration": 36,
    "start_time": "2022-06-01T19:52:52.610Z"
   },
   {
    "duration": 11,
    "start_time": "2022-06-01T19:52:56.411Z"
   },
   {
    "duration": 8,
    "start_time": "2022-06-01T19:53:00.524Z"
   },
   {
    "duration": 1928,
    "start_time": "2022-06-01T19:53:14.954Z"
   },
   {
    "duration": 147,
    "start_time": "2022-06-01T19:53:23.110Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-01T19:53:51.683Z"
   },
   {
    "duration": 9526,
    "start_time": "2022-06-01T19:53:53.659Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-01T19:54:14.963Z"
   },
   {
    "duration": 121,
    "start_time": "2022-06-01T19:55:17.855Z"
   },
   {
    "duration": 2,
    "start_time": "2022-06-01T19:55:43.166Z"
   },
   {
    "duration": 3,
    "start_time": "2022-06-01T19:56:04.528Z"
   },
   {
    "duration": 4,
    "start_time": "2022-06-01T19:57:49.521Z"
   },
   {
    "duration": 75930,
    "start_time": "2022-06-01T20:00:39.728Z"
   },
   {
    "duration": 0,
    "start_time": "2022-06-01T20:01:55.659Z"
   },
   {
    "duration": 1134,
    "start_time": "2022-06-01T20:02:55.507Z"
   },
   {
    "duration": 811,
    "start_time": "2022-06-01T20:02:56.643Z"
   },
   {
    "duration": 26,
    "start_time": "2022-06-01T20:02:57.455Z"
   },
   {
    "duration": 225,
    "start_time": "2022-06-01T20:02:57.483Z"
   },
   {
    "duration": 26,
    "start_time": "2022-06-01T20:02:57.709Z"
   },
   {
    "duration": 1977,
    "start_time": "2022-06-01T20:02:57.737Z"
   },
   {
    "duration": 218,
    "start_time": "2022-06-01T20:02:59.722Z"
   },
   {
    "duration": 9685,
    "start_time": "2022-06-01T20:02:59.941Z"
   },
   {
    "duration": 3,
    "start_time": "2022-06-01T20:03:09.628Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
